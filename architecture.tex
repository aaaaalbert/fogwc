%!TEX root = paper.tex
\section{System Architecture}

An important insight behind the design of Seattle as a platform
is how the classical
dichotomy of service platform operators and users gives way to multi-faceted
trust relationships between a large number of mutually-unrelated
stakeholders.
In Seattle, the actual edge-based software installation, core infrastructure
services, clearinghouse operations, platform software builds, and remote
application deployments might be carried out and managed by different
groups of people. Such people are potentially untrusted (or even unknown),
to members of the same group.
% Then why/how do people trust each other in such a system?

This gives rise to a set of components~\cite{Cappos2009} ---
any of which is useful
by itself --- that can be freely combined to implement various
deployment models, from pure peer-to-peer operation up to a
provisioned deployment by a dedicated operator. Furthermore,
new components may be introduced freely to replace or augment
existing ones, as long as all adhere to the component interfaces.
We briefly introduce the default component implementations below.
As detailed in Section~\ref{sec-deployment}, we have actively used
and developed these components over the last eight years as a part of
Seattle's public, live deployment.

\begin{figure}
  \centering
  \includegraphics[width=\columnwidth]{figures/components.pdf}
  \caption{Components and interconnections of Seattle's architecture.}
  \label{fig:arch}
\end{figure}


\subsection{Components}\label{sec-components}

The core components of Seattle and their interconnections are
show in Figure~\ref{fig:arch}.
%The components fall into three categories: device software,
%infrastructure, and control.
We first introduce the components running on all devices that
run Seattle.
For computation, the primary component is a restricted,
performance- and security-isolated Python-based
\textit{sandbox}~\cite{RepySandbox,li2015fence}.
The sandbox \gls{API} provides
networking, file, threading, and other functions to
the code it executes. Note that
due to Seattle's research and educational background, we
call code that is executed in sandboxes an ``experiment,''
regardless of its actual nature.
In the context of \gls{fc}, the sandbox performs distributed
computing and runs code to trigger actuators and read sensors.

The \textit{resource manager} combines the functionality of
a hypervisor with a remote-control server.
It isolates % chroots, effectively!
sandboxes that run in
parallel, and interfaces with them on behalf of authorized remote
parties to start, stop, or reset them, upload and download data,
or transfer ownership. The sandboxes on one
device can all have different authorized users if so desired,
so that one Seattle node can serve multiple experimenters.
Due to the heterogeneity encountered in today's networks,
the resource manager also includes techniques to traverse
\gls{NAT} gateways and firewalls as often found in end-user
gateway routers.

The \textit{device manager} % (installer with \texttt{--percentage}, start/stop scripts, uninstaller, softwareupdater): device owner retains control!
is the device owner's interface to enable or disable a Seattle
install, and choose the amount of resources that it may consume on the respective
device.

The \textit{software updater} keeps all components of the
device software up to date.
%This concludes the overview of
%device-side Seattle components.
%Other components are run either by experimenters, or
%infra-ops, or how should we say this so that it's exact?
%\\

An \textit{experiment manager} controls the fundamental functions of sandboxes, such as
starting the execution of code, or downloading collected data.
 % (seash)
The experiment manager programmatically contacts sandboxes
through the resource manager interface, and provides a
human-oriented interface to the experimenter. The experimenter
authenticates against sandboxes by using cryptographic keys.

All experimenters run an experiment manager, and possess
their own set of cryptographic keys. This enables interesting
deployment options (and thus applications).
Two experimenters could choose to mutually authenticate each
other on the sandboxes they control, or swap resources, and
so on. Thus, it makes more sandboxes available to an experiment than
a single experimenter could ever reach. Section~\ref{sec-trust}
explains in greater detail how Seattle manages trust relationships.
%\\

Seattle's device software and an initial set of cryptographic
keys controlling the sandboxes are shipped in the form of a bundled
installer for the specific platform \gls{OS}.
The \textit{installer builder} component is responsible for
packaging and hosting the code and public keys.
The installer builder allows experimenters to create their own
custom installers, where they get to choose what public key or keys
are to be included. Every custom installer
can be referenced through its specific, static \gls{URL}.
Thus, by selecting and installing a specific packaged installer,
device owners can choose which experimenters to initially grant
access to sandboxes on their devices.
%\\

%This sums up all of the Seattle components required for a
%self-hosted, infrastructure-less deployment already.

The default Seattle architecture also includes a
\textit{lookup service}. This is a generic network-accessible
key/value store that the resource manager and experiment manager
can use to announce and retrieve information about sandboxes.
For instance, a resource manager announces its contact information,
such as its own or a \gls{NAT} forwarder's \acrshort{IP} address and port number,
under the public keys of all currently authorized experimenters.
The experiment manager retrieves this contact information by looking up
the experimenter's public key at the lookup service, and then
proceeds to contact every resource manager that hosts sandboxes
the experimenter can use.

We use \textit{\gls{NAT} Relays} on some Seattle nodes that are
accessible from the public Internet. In this manner, firewalled
or \gls{NAT}ted nodes have a common, public meeting point to
which they can direct experiment managers.

Lastly, Seattle provides a \textit{clearinghouse}, which is a
centralized sandbox-sharing site. As mentioned earlier,
sandboxes can be swapped. A clearinghouse can act as a trusted
intermediary on behalf of all registered experimenters, thus creating
a large pool of available sandboxes without requiring all experimenters to
set up individual mutual trust relationships.
The clearinghouse uses the same resource manager interfaces as the
experiment manager, and provides a human-oriented interface through which
experimenters can request and release sandboxes.
The clearinghouse implements internal policies
that govern the exact rules for swapping sandboxes between
registered experimenters.

Our live deployment
of Seattle includes implementations of all components.
This also includes a
clearinghouse that offers free access to an initial set of sandboxes
for all registered experimenters.


\subsection{Roles And Operational Flexibility}\label{sec-op-flex}

The preceding section implicitly introduced different roles relevant
in the Seattle architecture:
\textit{device owners} control whether and how they participate
in Seattle, so as to provide sandboxes to the platform;
\textit{experimenters} use these sandboxes to run their code
on remote devices;
and, lastly, \textit{operators} manage various components
of the infrastructure.
These roles match what most existing \gls{fc} architectures
anticipate.
However, the limited and clearly-defined boundaries of Seattle
components result in few requirements for mutual trust or, indeed,
interaction between
the actual persons acting in these roles. This enables operational
scenarios including, but also exceeding, the often-assumed centrally managed
and operated \gls{fc} setups.

For instance, in a local experimental deployment, the owner of all
devices might also serve as an experimenter at the same time. If
the experiment managers and installer builders can be reused
from existing deployments, no additional effort is involved in
setting up the infrastructure.
A setup on exclusively-controlled, all-local devices can serve as
a testbed for research, or enable device owners to run an
``OwnCloud''-like \gls{fc} setup on their equipment.

The same approach also scales to groups of device owners and experimenters
who federate
to share resources on (parts of) their setups. No trusted
intermediary in the form of a clearinghouse is required.
Trust relationships among the researchers and device owners
can be set up out-of-band as well.

Another implementation could utilize all of the components described
in Section~\ref{sec-components} above.
Such a deployment would start with a sponsored set of sandbox resources,
a clearinghouse to mediate access between sponsored and contributed
sandboxes
and registered researchers, and all of the auxiliary services
discussed. One such example is the current deployment of Seattle. We operate
instances of all components, while further instances
(particularly of sandboxes on remote nodes) are contributed and
operated by volunteers.
Section~\ref{sec-deployment} details our current deployment.

Sensibility Testbed~\cite{zhuang2014sensibility} and Social Compute
Cloud~\cite{caton2014social} provide a similar breadth in capabilities
as Seattle. However, they only operate a partial infrastructure, and
reuse much of Seattle's. Their particular clearinghouse
implementations differ from Seattle's in terms of user interfaces
and policy implementations. Sensibility also provides a new sandbox
type that allows privacy-preserving access to sensors on Android
smartphones and tablets. Other components, including the experiment
manager, are reused without changes.

Finally, we envision that multiple central operators
could set up their own instances of
all components, and then federate using Seattle's open interfaces
to barter or trade resources. % With due thanks to Peter Reichl from whom I borrowed the formulation.
Operator-specific clearinghouses serve as centralized trust
anchors, and ``experiments'' would instead be ``apps'' for
end users, with the former experimenters as over-the-top service or
software providers.



\subsection{Trust Management}\label{sec-trust}

As detailed in the previous subsection, interpersonal or
institutional trust plays an important role in a Seattle-based
system. For instance, the device owner will only run an
installer package obtained from an installer builder that is
operated by a party they trust. Similarly, multiple clearinghouses
will only cooperate if mutual trust relationships can be
established.

Additionally, Seattle uses asymmetrical cryptography as
a technical means to map out trust.
Its core computational component, the sandbox, incorporates the roles of
an \textit{admin}
% actually called Owners in Seattle, but I wanted to keep the
% term distinct from "device owners" here.
and \textit{users}, all identified by
public keys added to the installer package by the
installer builder. The admin of a sandbox can add or remove
users, split the resources of a sandbox, or give up privileges
in favor of another admin (i.e., another public key).
An authorized user runs programs in the sandbox, transfers data,
and so on. If more than one user is authorized for the same
sandbox, they all share the same sandbox environment.

Depending on the deployment scenario realized, interpersonal
or institutional
and technical trust relationships align at different places
through interactions with multiple components in the system.
The first point is the installer builder, as the person configuring
the installer package chooses the initial set of admin and user public
keys going into it. Then, the two types of trust relationships align
again in the experiment manager, for example when an admin adds new user public
keys to sandboxes he controls. Finally, if the deployment uses a
clearinghouse to administer sandbox access, this becomes the
third point of alignment. The clearinghouse must trust an experimenter
(i.e. sandbox user) before placing the public key in sandboxes
it controls.

As a final remark, the software updater component of the
Seattle installer also contains a public key. It is used to
verify the integrity of updates. This key is likely managed by
the operator of the installer builder who controls the software
that is packaged. However, there is no intrinsic requirement for this
to be the case. A trust relationship may extend to
this component as well.
